{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xor\n",
    "\n",
    "Now let's try a slightly harder example - the XOR function. Unlike AND or OR, it cannot be solved via a single perceptron, because it is not \"linearly separable\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xor is true (1 in this case) only when one of the inputs is true. If both or neither is true, then the result is false (or 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = np.array([\n",
    "    np.array([1, 1]),\n",
    "    np.array([1, 0]),\n",
    "    np.array([0, 1]),\n",
    "    np.array([0, 0])\n",
    "])\n",
    "\n",
    "training_labels = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(layer_dimensions: List[int]):\n",
    "    parameters = {}\n",
    "    numLayers = len(layer_dimensions)\n",
    "    \n",
    "    for l in range(1, numLayers):\n",
    "        # Initialize weights to a small random number.\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dimensions[l], layer_dimensions[l-1]) * 0.01\n",
    "        # Initialize biases to zero.\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dimensions[l], 1))\n",
    "        \n",
    "        # Ensure that everything looks right.\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dimensions[l], layer_dimensions[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dimensions[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    The \"linear\" part of a layer's forward propagation.\n",
    "    \n",
    "    Arguments::\n",
    "    A -- Activations from the previous layer.\n",
    "    W -- Weights matrix.\n",
    "    b -- Bias vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input to the activation function.\n",
    "    Z = np.dot(W, A) + b\n",
    "    # Values we'll need when doing backward propagation.\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A, Z\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    A, activation_cache = activation(Z)\n",
    "    \n",
    "    assert(A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Forward propagation for each layer. Each layer is Linear->Relu, except\n",
    "    for the output layer, which is Liner->Sigmoid.\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    # The initial \"previous\" layer is the inputs.\n",
    "    A = X\n",
    "    # The number of layers. Note that we divide by because the `parameters` contains\n",
    "    # weights AND biases for each layer.\n",
    "    numLayers = len(parameters) // 2\n",
    "    \n",
    "    # Relu layers\n",
    "    for l in range(1, numLayers):\n",
    "        A_prev = A\n",
    "        W = parameters['W' + str(l)]\n",
    "        b = parameters['b' + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, relu)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    # Sigmoid layer\n",
    "    W = parameters['W' + str(numLayers)]\n",
    "    b = parameters['b' + str(numLayers)]\n",
    "    Yhat, cache = linear_activation_forward(A, W, b, sigmoid)\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(Yhat.shape == (1, X.shape[1]))\n",
    "    \n",
    "    return Yhat, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Yhat, Y):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy cost.\n",
    "    \n",
    "    Arguments:\n",
    "    Yhat -- Probability vector corresponding to label predictions.\n",
    "    Y -- Actual label vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    numExamples = Y.shape[1]\n",
    "    \n",
    "    cost = -(1 / numExamples) * np.sum(np.multiply(Y, np.log(Yhat)) + np.multiply(1 - Y, np.log(1 - Yhat)))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Linear part of backwards propagation.\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- Derivative (gradient) of the cost with respect to the linear output of the current layer.\n",
    "    cache -- Tuple of values (A_prev, W, b) coming from forward propagation of the current layer.\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Derivative of the cost with respect to the activation of the previous layer.\n",
    "    dW -- Derivative of the cost with respect to the weights of the current layer.\n",
    "    db -- Derivative of the cost with respect to the bias of the current layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    numExamples = A_prev.shape[1]\n",
    "    \n",
    "    dA_prev = np.dot(np.transpose(W), dZ)\n",
    "    dW = (1 / numExamples) * np.dot(dZ, np.transpose(A_prev))\n",
    "    db = (1 / numExamples) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation_backward):\n",
    "    linear_cache, activation_cache = cache\n",
    "    dZ = activation_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, Z):\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    # When Z <= 0, dZ should be 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_backward(Yhat, Y, caches):\n",
    "    \"\"\"\n",
    "    Backward propagation for each layer (starts with the last layer).\n",
    "    \n",
    "    Arguments:\n",
    "    Yhat -- Probability vector. The output of forward propagation.\n",
    "    Y -- Labels.\n",
    "    caches -- List of caches captured from each forward propagation step.\n",
    "    \"\"\"\n",
    "    \n",
    "    gradients = {}\n",
    "    num_layers = len(caches)\n",
    "    num_examples = Yhat.shape[1]\n",
    "    Y = Y.reshape(Yhat.shape)\n",
    "    \n",
    "    # Derivative of the cost with respect to Yhat.\n",
    "    dYhat = -(np.divide(Y, Yhat) - np.divide(1 - Y, 1 - Yhat))\n",
    "    \n",
    "    # The last (sigmoid) layer.\n",
    "    current_cache = caches[num_layers - 1]\n",
    "    gradients['dA' + str(num_layers)], gradients['dW' + str(num_layers)], gradients['db' + str(num_layers)] = linear_activation_backward(dYhat, current_cache, sigmoid_backward)\n",
    "    \n",
    "    for l in reversed(range(num_layers - 1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(gradients[\"dA\" + str(l + 2)], current_cache, relu_backward)\n",
    "        gradients['dA' + str(l + 1)] = dA_prev_temp\n",
    "        gradients['dW' + str(l + 1)] = dW_temp\n",
    "        gradients['db' + str(l + 1)] = db_temp\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(parameters, gradients, learning_rate):\n",
    "    num_layers = len(parameters) // 2\n",
    "    for l in range(num_layers):\n",
    "        parameters['W' + str(l + 1)] = parameters['W' + str(l + 1)] - learning_rate * gradients['dW' + str(l + 1)]\n",
    "        parameters['b' + str(l + 1)] = parameters['b' + str(l + 1)] - learning_rate * gradients['db' + str(l + 1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layer_dimensions, learning_rate = 0.0075, num_iterations = 3000):\n",
    "    costs = []\n",
    "    parameters = initialize_params(layer_dimensions)\n",
    "    \n",
    "    # Gradient descent\n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward propagation\n",
    "        Yhat, caches = model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(Yhat, Y)\n",
    "        \n",
    "        # Backward propagation\n",
    "        gradients = model_backward(Yhat, Y, caches)\n",
    "        \n",
    "        # Update parameters\n",
    "        parameters = update_params(parameters, gradients, learning_rate)\n",
    "        \n",
    "        costs.append(cost)\n",
    "        \n",
    "    # Plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = model(training_inputs.T, training_labels.reshape(1, -1), [2, 3, 1], learning_rate = 0.05, num_iterations = 5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
